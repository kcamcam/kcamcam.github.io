<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=-lTUqgJg2dxbe4D7B5DEIA3jn2WilaVUapNOYl4762s');.lst-kix_oz3l4i5d4rj3-6>li:before{content:"\0025cf  "}.lst-kix_oz3l4i5d4rj3-7>li:before{content:"\0025cb  "}.lst-kix_oz3l4i5d4rj3-5>li:before{content:"\0025a0  "}ul.lst-kix_oz3l4i5d4rj3-2{list-style-type:none}ul.lst-kix_oz3l4i5d4rj3-3{list-style-type:none}ul.lst-kix_oz3l4i5d4rj3-4{list-style-type:none}ul.lst-kix_oz3l4i5d4rj3-5{list-style-type:none}ul.lst-kix_oz3l4i5d4rj3-6{list-style-type:none}.lst-kix_oz3l4i5d4rj3-0>li:before{content:"\0025cf  "}ul.lst-kix_oz3l4i5d4rj3-7{list-style-type:none}.lst-kix_oz3l4i5d4rj3-8>li:before{content:"\0025a0  "}ul.lst-kix_oz3l4i5d4rj3-8{list-style-type:none}.lst-kix_oz3l4i5d4rj3-1>li:before{content:"\0025cb  "}ul.lst-kix_oz3l4i5d4rj3-0{list-style-type:none}ul.lst-kix_oz3l4i5d4rj3-1{list-style-type:none}.lst-kix_oz3l4i5d4rj3-2>li:before{content:"\0025a0  "}.lst-kix_oz3l4i5d4rj3-3>li:before{content:"\0025cf  "}.lst-kix_oz3l4i5d4rj3-4>li:before{content:"\0025cb  "}ol{margin:0;padding:0}table td,table th{padding:0}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Montserrat";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Montserrat";font-style:normal}.c2{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Montserrat";font-style:normal}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c31{color:#000000;font-weight:700;text-decoration:none;font-size:12pt;font-family:"Montserrat";font-style:normal}.c35{color:#000000;font-weight:400;text-decoration:none;font-size:11pt;font-family:"Arial";font-style:normal}.c18{padding-top:18pt;padding-bottom:5pt;line-height:1.15;page-break-after:avoid;text-align:left}.c28{padding-top:14pt;padding-bottom:5pt;line-height:1.15;page-break-after:avoid;text-align:center}.c20{padding-top:18pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c10{padding-top:14pt;padding-bottom:5pt;line-height:1.15;page-break-after:avoid;text-align:left}.c3{padding-top:0pt;padding-bottom:5pt;line-height:1.15;text-align:justify}.c9{padding-top:0pt;padding-bottom:5pt;line-height:1.15;text-align:left}.c11{color:#000000;text-decoration:none;vertical-align:baseline;font-style:italic}.c1{vertical-align:super;font-size:9pt;font-family:"Montserrat";font-weight:400}.c26{color:#000000;text-decoration:none;vertical-align:super;font-style:normal}.c19{font-size:10pt;font-family:"Montserrat";font-weight:400}.c27{font-size:9pt;font-family:"Montserrat";font-weight:400}.c23{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c36{font-weight:700;font-size:14pt;font-family:"Arial"}.c4{font-size:12pt;font-family:"Montserrat";font-weight:400}.c37{color:#000000;text-decoration:none;font-style:normal}.c25{padding:0;margin:0}.c17{margin-left:36pt;padding-left:0pt}.c21{font-style:italic}.c7{text-indent:19.4pt}.c22{vertical-align:baseline}.c32{text-indent:14.6pt}.c6{text-indent:18.7pt}.c33{vertical-align:super}.c29{height:11pt}.c14{text-indent:19pt}.c5{text-indent:19.2pt}.c34{text-indent:18.5pt}.c8{text-indent:19.7pt}.c24{margin-left:36pt}.c30{text-indent:0.2pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c23"><p class="c9"><span class="c22 c31">NORTH- HOLLAND</span></p><h2 class="c18" id="h.t3dg39kezigq"><span class="c13">Technology Assessment: Product or Process?</span></h2><p class="c9"><span class="c0">JOS</span><span class="c4">EE</span><span class="c0">&nbsp;C. M. VAN EIJNDHOVEN</span></p><p class="c9 c29"><span class="c0"></span></p><p class="c9"><span class="c0">ABSTRACT</span></p><p class="c3 c32"><span class="c12">Technology assessment was originally conceived of as an analytic activity, aimed at providing decision makers with an objective analysis of effects of a technology. Early in the history of technology assessment, it became clear that assessment projects must involve multiple perspectives. In the United States, this led to stakeholder involvement in the analysis. In a number of European countries, however, forms of technology assessment developed in which the analytic product became of relatively minor importance compared to the interactive process: consensus conferences and constructive technology assessment developed as alternative forms. This article discusses four paradigms of technology assessment: the classical paradigm: the Office of Technology Assessment (OTA) paradigm; public technology assessment; and constructive technology </span><span class="c27">assessment</span><span class="c12">. It concludes that the multiple views of technology assessment and its position between academia and politics lead to dilemmas for technology assessment organizations, especially after the demise of OTA. It stresses the importance of experimenting with various ways of conducting technology assessments and of addressing quality control. &copy; 1997 Elsevier Science Inc.</span></p><h2 class="c20" id="h.dot5yvcrg518"><span class="c13">Introduction</span></h2><p class="c3 c6"><span class="c0">Technology assessment for the </span><span class="c4">legislative</span><span class="c0">, or &quot;parliamentary technology </span><span class="c4">assessment</span><span class="c0">,&quot; was institutionalized in the United States in 1972 and later in several European countries. The largest wave of institutionalization in Europe occurred in the mid-1980s. The way technology assessment developed in these arenas bears the mark not only of the period in which it was conceptualized and the specific surroundings in which it has to operate, but also of the way in which technology development, the relationship between technology and democracy, and technology policy are conceived.</span></p><p class="c3 c14"><span class="c0">In a study of the development of technology assessment in six countries (the United States, Sweden, the Federal Republic of Germany, the United Kingdom, The Netherlands, and France) Smits and Leijten [1] identified eight functions that technology assessment was supposed to fulfill (see Table 1). From their analysis, Smits and Leijten concluded that not all of these functions received the same emphasis over time. Smits [2] noted that the early warning function had to take a step back in virtually all the countries studied in favor of supporting technology policy. In the first half of the 1980s a strong increase in attention to broadening the decision-making process and constructive technology assessment was visible in Europe. Finally, he found that although initiating</span></p><h3 class="c28" id="h.dks1gfiikl2l"><span class="c2">TABLE 1 </span></h3><h3 class="c28" id="h.dks1gfiikl2l-1"><span class="c22">Eight Functions Attributed to Technolo</span><span>g</span><span class="c2">y Assessment</span></h3><p class="c9"><span class="c15">1. Fortification of the position in decision making. This mainly refers to technology assessment initiatives instigated by parliaments but also other political and administrative actors who attempt to obtain a stronger influence on decision making by widening their sources of information in respect of scientific and </span><span class="c19">technological</span><span class="c15">&nbsp;developments. </span></p><p class="c9"><span class="c15">2. Support of the short- and medium-term policy (executive branch and the legislature) and within the </span><span class="c19">framework</span><span class="c15">&nbsp;of the current policy, suggesting aspects such as control, exploration of alternatives, evaluation and, not infrequently, legitimation. </span></p><p class="c9"><span class="c15">3. Contributing to the initiation and development of long-term policy by providing information about possible</span><span class="c19">&nbsp;</span><span class="c15">developments and alternatives. </span></p><p class="c9"><span class="c15">4. Early warning, aimed at providing information about possible problematic and undesirable consequences</span><span class="c19">&nbsp;</span><span class="c15">of technological development at the earliest possible stage. </span></p><p class="c9"><span class="c15">5. Expanding knowledge and decision making about technology by giving support to societal groups as regards</span><span class="c19">&nbsp;</span><span class="c15">the formulation of their own strategy with respect to technological developments, 6. Tracking down, formulating, and developing desirable and useful technological applications for society</span><span class="c19">&nbsp;</span><span class="c15">(constructive technology assessment). </span></p><p class="c9"><span class="c15">7. Encouraging the general public to accept technology. 8. Promoting scientists&#39; awareness of their social responsibility.</span></p><p class="c9"><span class="c11 c19">Source: [1].</span></p><p class="c3"><span class="c0">and developing future technology policy is given a great deal of attention in discussions, this attention is not reflected in the practice of technology assessment. Smits and Leijten completed their study in the second half of the 1980s and, although it was not specifically directed at parliamentary technology assessment, the list of functions still provides an overview of the types of considerations playing a role in thinking about it.</span></p><p class="c3 c5"><span class="c0">In the first section, this article discusses technology assessment as performed by the Office of Technology Assessment (OTA) and its European counterparts, thereby limiting itself to legislative or parliamentary technology assessment. In the second section, it addresses four perspectives or paradigms of technology assessment, the </span><span class="c4">associated</span><span class="c0">&nbsp;methods used by these organizations, and some of the background for these choices. Finally, it discusses the lessons that can be drawn for technology assessment in the political realm from dilemmas in the performance of technology assessment and inferred from its institutionalization and these paradigms.</span></p><h2 class="c18" id="h.klg8prep96jc"><span class="c13">Development of Parliamentary Technology Assessment</span></h2><p class="c3 c14"><span class="c0">In this section I present a brief overview of the emergence of the technology assessment organizations in the United States and Europe that are linked to parliaments (Congress in the United States), emphasizing the timing of their genesis, their main mission, and their main institutional characteristics. 1</span></p><p class="c3 c6"><span class="c0">The need for technology assessment started to be felt in the late 1960s when a number of large technological projects were proposed and met huge resistance. Smits and Leijten [1] point to three factors that were of major importance in the articulation of a need to assess technologies: (1) the concern about the consequences of new </span><span class="c4">technologies</span><span class="c0">, visible in the upsurge of environmental and anti-nuclear movements; (2) the need for</span><span class="c4">&nbsp;</span><span class="c11 c4">ex ante </span><span class="c0">assessment of large governmental technological projects; and (3) the demand for more involvement by stakeholders and members of the public. The general demand for assessing technological developments led to numerous activities in which one or another kind of assessment of technology took place, and a number of rules were put into place that necessitate assessment. Examples include obligations to provide environmental impact statements before certain projects are started, assessment </span><span class="c4">processes</span><span class="c0">&nbsp;of medical technologies, and initiatives to enhance citizen participation.</span></p><p class="c3 c34"><span class="c0">The concern about the consequences of new technologies as expressed by citizens and action committees was often not shared by pressure groups having a stake in developing the technology in question (to put it mildly) and, as a consequence, different groups presented widely diverging views of the likely effects of technology. In a number of parliaments this abundance of interested perspectives led to the wish to be provided with a more balanced view of technology and its likely effects. In the United States and several other countries, another important factor was the disequilibrium felt between the powers of the executive branch and of the legislature [3]. The growing expertise on both sides of the issues and the one-sided information provided by the executive branch, led to a desire in the U.S. Congress to get more objective information on the consequences of technology at an early stage. This &quot;early warning system&quot; was intended to enable decision makers to avoid unwanted side effects of new technologies.</span></p><p class="c3 c14"><span class="c0">The U.S. Congress established the Office of Technology Assessment in 1972 to provide itself with objective information about the secondary effects of technology and thereby to enable it to assess independently the virtues of envisaged technological developments and redress the imbalance between legislature and executive. As has been related numerous times, the first years of OTA are not considered to have been very successful, primarily because the institution failed to achieve a scientific quality of its reports that was beyond suspicion and evade unwanted politicization [1]. After the initial directorship of former Representative Emilio Daddario, OTA changed its operational procedures so that the products of its assessments were thoroughly reviewed internally and externally by advisory committees and others. OTA succeeded in </span><span class="c4">becoming</span><span class="c0">&nbsp;viewed as an organization providing neutral, objective information.</span></p><p class="c3 c7"><span class="c0">OTA preceded the birth of parliamentary technology assessment organizations in Europe by more than ten years. In the 1980s, six parliamentary technology assessment organizations were started in Europe~in France, the Netherlands, Denmark, the United Kingdom, Germany, and one connected to the European Parliament. All have their own history and organizational structure, reflecting the pressures that played a role in their start-up process as well as the cultural differences in the political process in each of the countries [4].</span></p><h3 class="c10" id="h.5j7llccth3fk"><span class="c2">FRANCE</span></h3><p class="c3 c34"><span class="c0">The first of the European parliamentary TA organizations was the Office </span><span class="c4">Parlementaire</span><span class="c0">&nbsp;d&#39;Evaluation des Choix Scientifiques et Technologiques (OPECST). Of all the parliamentary technology assessment organizations, this institution is the one most intimately linked with parliament, because it is the parliamentarians themselves who conduct the assessments. OPECST has only the function to support the parliamentarians conducting analyses.</span></p><p class="c3 c14"><span class="c0">In the original design of the organization, it was suggested that an organizational and procedural division be created between two phases: in-depth analysis of technological developments by scientific experts and a second phase assigned to political leaders in charge of sorting out the conclusions of the experts&#39; analysis. But it was quickly concluded that such a division of labor could not work, because translation of scientific results into policy relevant conclusions is a politically laden activity that requires in-depth</span><span class="c4">&nbsp;</span><span class="c0">knowledge of the issues. It therefore was concluded that assessment activities should be conducted by parliamentarians themselves, supported by staff. Information gathering takes place mainly via public and private hearings. 2</span></p><h3 class="c10" id="h.9ixg77az4o9x"><span class="c2">NETHERLANDS</span></h3><p class="c3 c6"><span class="c0">Three other parliamentary technology assessment organizations began life in 1986, in the Netherlands, Denmark, and that of the European Parliament. The Netherlands Office for Technology Assessment (NOTA) was created by a ministerial decree of June 17, 1986, as a result of deliberations on a government white paper on the integration of science and technology in society [5]. The context of this white paper was the necessity felt to broaden the basis for decision making on science and technology in Dutch society, to better anticipate societal consequences. The government originally suggested housing technology assessment within the Ministry of Education and Science. But </span><span class="c4">parliamentarians</span><span class="c0">&nbsp;stressed the need for an independent body &quot;at arm&#39;s length&quot; from government, in order to ensure that it would not necessarily speak with the voice of government or any minister.</span></p><p class="c3 c6"><span class="c0">The decision was made to install NOTA as an independent body. No formal link to parliament was created, but parliament had to approve the work program and it is considered to be the primary client of the organization. The client relationship developed slowly, because in its early years multiple interpretations of the primary client existed within the organization and in the surrounding world. An evaluation in 1993 [6] </span><span class="c4">concluded</span><span class="c0">&nbsp;that NOTA&#39;s activities were too scientifically oriented and that NOTA should stress its advisory role and its role in debate more than was the case. As a consequence of the evaluation, the name of the organization was changed to the Rathenau Institute and the mission from performing a program of technology assessments to the support of decision making and societal debate. 3</span></p><h3 class="c10" id="h.puyz2o5fk0h5"><span class="c2">DENMARK</span></h3><p class="c3 c14"><span class="c0">The Danish Board of Technology moved in the opposite direction from the </span><span class="c4">development</span><span class="c0">&nbsp;in the Netherlands. The Board originally had a direct link to the Danish parliament (the Folketing) in the form of a specific parliamentary committee, consisting of nine of its members. The Board had the task to monitor and initiate comprehensive assessments of technologies, but it also had the task to foster public debate on the scope and implications of technology [7]. During the first nine years of its existence, the Board was formally not a permanent body. Although--and even possibly because of--its activities, including numerous consensus conferences, were considered very successful, the debate on the eventual establishment of the Board as a permanent body was lengthy and involved discussions on the relationship between the Board and parliament. In the end, the formal task of the Board did not change: it kept its mission to monitor </span><span class="c4">technological</span><span class="c0">&nbsp;development, to carry out comprehensive assessments of the possibilities and impacts of technology, and to further the public debate about technology [8]. However, the direct link with parliament in the form of its parliamentary committee was dissolved, and the Board is now linked to the Minister of Research.</span></p><h3 class="c10" id="h.jujbsfsd47er"><span class="c2">EUROPEAN UNION</span></h3><p class="c9 c14"><span class="c0">Scientific and Technological Options Assessment (STOA), the technology </span><span class="c4">assessment</span><span class="c0">&nbsp;body of the European Parliament, was originally launched as an 18-month pilot</span><span class="c4">&nbsp;</span><span class="c0">project under the auspices of the Committee on Energy, Research, and Technology of the European Parliament in March 1987. It was based on a report by Roll Linkohr, a German member of the European Parliament (EP), who also became the first chair of STOA. Its structure consisted of two parts: the STOA panel and the STOA team. The STOA panel is a working party of EP members who decide on the projects for the year and make other major political decisions. The STOA team comprises EP Secretariat officials and experts on short-term contracts [9].</span></p><p class="c3 c14"><span class="c0">Parliamentarians conceived of STOA as an agency that should be able to provide objective, comprehensive, and independent assessments of scientific and technological issues that would benefit the EP as a whole [3]. One of the incentives to create STOA was to put the EP on a more equal footing with the European Commission &quot;when it comes to such important matters as the social, economic, or environmental impact of the introduction of new technologies&quot; (quoted in [3]). STOA operations were evaluated in October 1994 by a team directed by William Westermeyer from OTA. The evaluation report concluded that STOA had encountered difficulties in implementing the intentions of its creators because of uncertainty over how the EP as a whole could make effective use of STOA&#39;s potential [3]. A number of reasons were given for this state of affairs, including ambiguities in </span><span class="c4">STOYA&#39;s</span><span class="c0">&nbsp;mission and the guidance given by the STOA panel, but also the difficulty of exercising quality control with little permanent staff and diverging expectations. The evaluation by the Westermeyer committee was followed by other, more or less formal evaluative activities, which have not led to a final conclusion to date.</span></p><h3 class="c10" id="h.6k98zraq0sf3"><span class="c22">UNITED KINGDOM </span><span class="c26 c36">4</span></h3><p class="c3 c34"><span class="c0">The UK Parliamentary Office of Science and Technology (POST) started in 1989 as a charitable trust outside Parliament, but a number of Parliamentarians were among the benefactors of the trust. 5 Because of limited financial means, POST originally </span><span class="c4">emphasized</span><span class="c0">&nbsp;the production of brief factual reports and conducted only occasionally in-depth analyses that could be considered technology assessments. POST started to receive parliamentary funding in 1993, initially for a period of three years. In 1996 POST became a permanent body in Parliament. In the course of its existence the POST Briefing Notes remained the most permanent activity, but the number of full reports grew over time. The role of POST is slowly shifting from an external information function to a role as a more direct supporter of parliamentary committees.</span></p><h3 class="c10" id="h.vi6op5xgk4qf"><span class="c2">GERMANY</span></h3><p class="c3 c14"><span class="c0">The start-up period of the German parliamentary technology assessment activity was the most extended of all those related here [10]. The first proposal to establish an OTA-type body in the Bundestag was made in April 1973. At the end of 1984, the Research Committee of the Bundestag decided to set up a Committee of Inquiry on &quot;Technology Assessment and Evaluation.&quot; This body was made up of members of Parliament and experts. The committee (which operated in two phases: 1985-1987 and 1987-1990) conducted inquiries into a number of technological areas (partly in the interest of providing models for future work) and developed proposals for future </span><span class="c4">institutionalization</span><span class="c0">. Partly on the basis of these proposals, it was decided in 1989 that a</span><span class="c4">&nbsp;</span><span class="c0">&quot;Bundestag</span><span class="c4">&nbsp;</span><span class="c0">Technology Assessment Bureau&quot;</span><span class="c4">&nbsp;</span><span class="c0">(Technikfolgenabschaetzungsbuero Deutscher Bundestag, TAB) would be set up in Bonn. Its activities would be conducted by an external research institute, led by Professor Paschen of the Department of Applied Systems Analysis (AFAS) of the Karlsruhe Nuclear Research Centre. This institute was chosen from 25 applicants. The Bundestag Committee of Research and Technology was renamed the Committee for Research, Technology, and Technology Assessment and became responsible for initiating and politically directing the analyses [11]. After a three-year provisional period, the German Parliament unanimously decided on March 4, 1993, to transform TAB into a permanent institution working for the German </span><span class="c4">Parliament</span><span class="c0">. For the period 1993 to 1998, AFAS was again contracted to run TAB [12].</span></p><h3 class="c10" id="h.eygyvxuhzje1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 627.94px; height: 314.50px;"><img alt="" src="images/image1.png" style="width: 663.00px; height: 314.50px; margin-left: -15.94px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h3><h3 class="c10" id="h.nmqk3m29hwon"><span class="c2">COMPARISON</span></h3><p class="c3 c5"><span class="c0">If we compare the European parliamentary technology assessment organizations, it is remarkable that all of them have about the same order of magnitude. The size of each, as measured by budget and personnel, is about one tenth of </span><span class="c4">ONE&#39;s</span><span class="c0">&nbsp;former volume. Table 2 shows the budget and staffing for these organizations in 1990. (It should be realized that the French parliamentarians are not included in the numbers.)</span></p><p class="c3 c14"><span class="c0">A number of observations can be made from these short histories of technology assessment organizations. One issue that apparently plays a role is the tension inherent in the choice of where to locate the activities in terms of distance from the political process. In France, politicians are in charge of conducting the assessments; whereas, in the Netherlands a relatively large distance exists between the activities of the technology assessment organization and parliament. The lack of clarity about the role of parliament vis-~</span><span class="c4">a</span><span class="c0">-vis government in Europe has been viewed by some as one of the reasons for the relatively long birth process of some technology assessment organizations [10, 11]. An additional reason is that in many European countries, shifting coalitions may favor or disfavor the institutionalization of such an organization. Variations in the roles and strengths of parliaments also play a role in the way technology assessment organizations become established. Some (e.g., in the Netherlands) have little fact-finding traditions of their own; whereas, others (e.g., Germany) do have a tradition of conducting their own analyses. It may also be important whether the parliament has the tradition of conducting its own analyses in permanent or temporary committees, or mainly reacting to government initiative.</span></p><p class="c3 c6"><span class="c0">A number of other factors or tensions common to the development of technology assessment organizations have been stressed much less, but they seem to be at least as important as differences in political culture or the institutionalization of political systems. These common factors--related to the nature of technology assessment as an activity and reflected in a similarity in the history of almost all European technology assessment organizations--are the tensions in the organization&#39;s relationship to academia. The French, German, Dutch, and Danish cases show how this tension influenced </span><span class="c4">institutionalization</span><span class="c0">. In France, the eventual conviction that conducting academic analyses and </span><span class="c4">drawing</span><span class="c0">&nbsp;politically relevant conclusions are inseparable, led to an organization centered around parliamentarians. In Germany, the initial, mixed parliamentary-expert </span><span class="c4">committee</span><span class="c0">&nbsp;(the same type of composition as in German &quot;Enquete-</span><span class="c4">Commission</span><span class="c0">&quot;) was, after the initial phase, replaced by an external research institute to conduct analyses and a steering body within parliament. The Danish parliamentary committee, steering the Danish Board, was replaced by a closer link to the Minister of Research. And in the Netherlands, the anticipated relationship to the Minister of Education and Science was replaced by a relationship to the Royal Netherlands Academy of Arts and Sciences. This connection in its turn was found to lead to too strong an orientation toward academia [6] and too weak a relationship to the political process. The new mission and the new name sought to remedy these shortcomings. In all these cases, the present set-up has stabilized into one that tries to keep a delicate balance between politics and academia.</span></p><p class="c3 c14"><span class="c0">A second issue that returns regularly in the debate about the value of technology assessment is the need for quality control for the products of technology assessment. The history of OTA shows how important it is that assessments be viewed as </span><span class="c4">authoritative</span><span class="c0">&nbsp;to be effective. The experience so far seems to show that more is needed than the usual academic review procedure for a technology assessment to gain credibility, because in many cases they address highly sensitive issues, of which the treatment has to be viewed as even-handed by parties whose views are widely divergent.</span></p><p class="c3 c5"><span class="c0">A third issue that is visible from the short history of the institutionalization of technology assessment is the tension between a role in supporting parliament versus a wider role in supporting public debate on science and technology in society. Especially in Denmark and the Netherlands, this second mission is stressed over the first.</span></p><p class="c3 c14"><span class="c0">We will return to the issues of distance from the political process, the relationship to academia, and the question of support to which societal processes in the final section, after a discussion of the various perspectives on technology assessment.</span></p><h3 class="c10" id="h.x8dvdvwtjrdr"><span class="c2">Technology Assessment Paradigms</span></h3><p class="c3 c14"><span class="c0">Opinions of what technology assessment is or should be continue to be very diverse. As indicated in the introduction, a large number of different practical functions have been ascribed to technology assessment [1], and such functions have also been discussed in terms of underlying viewpoints of its role in (democratic) society [14, 15].</span></p><p class="c3 c5"><span class="c0">If we try to sketch a broad outline of the general development of technology assessment in the political realm over the last 20 or so years, we can discern a number of approaches that have gained some authority in international debate. I call these approaches the classical technology assessment paradigm, the OTA paradigm (and its derivatives), public technology assessment, and constructive technology assessment.</span></p><h3 class="c10" id="h.zcrg9glfljem"><span class="c2">CLASSICAL TECHNOLOGY ASSESSMENT PARADIGM</span></h3><p class="c9 c6"><span class="c0">When technology assessment originated almost three decades ago, the idea was that it should study the likely secondary impacts of technology to provide decision</span><span class="c4">&nbsp;</span><span class="c0">makers with objective information on those impacts. This idea is the classical technology assessment paradigm. A typical and much cited definition of technology assessment from the early period reads [16].</span></p><p class="c3 c24 c30"><span class="c12">Technology assessment is the systematic identification, analysis and evaluation of the potential secondary consequences (whether beneficial or detrimental) of technology in terms of its impacts on social, cultural, political and environmental systems and processes. Technology assessment is intended to provide a neutral, factual input to decision making.</span></p><p class="c3 c14"><span class="c0">By providing decision makers with information on the likely future effects of a technology, technology assessment was supposed to fulfill an early warning function. The idea was then to avoid unwanted impacts by acting according to the insights provided by the assessment. However, two major problems arose. First, it became clear in practice, as well as from theoretical considerations, that the impacts of technology can only very partially be foreseen [17]. Second, it was shown to be more problematic than originally thought to address the issue of impacts impartially; technology assessments did not provide decision makers with neutral, let alone objective, information [1].</span></p><h3 class="c10" id="h.jrgl8jc9m55h"><span class="c2">OTA PARADIGM</span></h3><p class="c3 c6"><span class="c0">For these reasons, initial technology assessments were less useful than was hoped for. Reflections on how to solve such problems in relationship to the operations of the U.S. Office of Technology Assessment led to what I call the OTA paradigm. After an initial phase, OTA assessments were no longer so much directed at early warning, but at the development of policy alternatives. Three interrelated factors were crucial for the perceived success of such assessments: the committee primacy principle, stakeholder involvement, and quality control of the final reports. All of these factors augmented the credibility of the assessments and rendered them less dependent upon influences that might be biased [1].</span></p><p class="c3 c5"><span class="c0">OTA&#39;s projects were conducted on the basis of a formal request from one or more congressional committees. This committee primacy principle assures that there is a direct link between the operations of the technology assessment organization and the political process, while at the same time guaranteeing that no clientele relationship develops between certain politicians and staff members. Stakeholder involvement (and other forms of participation by people outside the direct operations of Congress or OTA) was one method OTA developed to counter accusations of bias, especially in defining issues. OTA recognized that defining key issues is one of the most crucial phases in an assessment, and it took an appreciable amount of time and involved many outside persons. In the words of Mary Procter [18]:</span></p><p class="c3 c24 c30"><span class="c12">During this phase, the project team, through intense interaction with the Congressional clients (usually committee staff) and with the advisory panel of stakeholders, identifies the key issues that are of policy concern in the assessment .... During its entire first meeting, members of a study&#39;s advisory panel (a group of 15-20 stakeholders that guides the study) probe and tear at the study plan. Panels involve at least two-thirds non academics, practical people involved in business, environmental action, labor unions. Such people are quick to sniff out an interesting but irrelevant topic that may be fun to study but is of no apparent use to policy making.</span></p><p class="c3 c14"><span class="c0">The third critical factor was a high quality review system. All OTA reports under- went a thorough internal and external review process. Each full-scale assessment had an advisory panel consisting of stakeholders and other experts. Drafts of final reports were subjected to an extensive external review procedure. And many times additional workshops were organized in which (parts of) assessments were discussed [1, 19].</span></p><p class="c3 c6"><span class="c0">The fruitfulness (and the limitations, for that matter 6) of the approach sketched here were very much determined by the intimate relationship of OTA to its client, the U.S. Congress. Two specific aspects of this relationship are important here. First is the fact that Congress plays a far more important role in legislative and budgetary matters than European parliaments. In the OTA paradigm, illuminating the possible alternative starting points for new legislative activities is something that a technology assessment can provide. Second, not only is the separation of powers (among the legislative branch, the executive branch, and the judiciary) very strict in the United States, but it is also the norm rather than the exception that each branch has its own support structure.</span></p><p class="c3 c14"><span class="c0">A number of these conditions do not hold in the public system in Europe. For instance, the legislative role of parliaments is in practice much weaker than that of the U.S. Congress. In parliamentary systems, the separation of powers between the </span><span class="c4">legislative</span><span class="c0">&nbsp;and the executive is much less strict than in the United States; in the United Kingdom, for example, members of the government are parliamentarians at the same time. Also, in most political cultures advocacy roles of expertise for one party or another are less strict. For example, in the Netherlands, although the Health Council is an advisory body to a number of ministers who deal with health related activities, parliament may and will occasionally ask the Council for advice (this is formally routed via one of the ministers). As a consequence, it would not, for instance, be considered appropriate for an organization such as the Rathenau Institute to send a report to parliament on an issue that the Health Council just dealt with, unless it is very clear that there is a difference in approach and conclusions, or if it can be considered a commentary.</span></p><p class="c3 c6"><span class="c0">The crucial characteristics of the OTA paradigm can be summarized as in-depth assessments leading to reports that provide a thoroughly informed analysis of a policy area in a scientifically valid, nonpartisan way, providing options for policy development. generated in a process involving stakeholders and tied in particular ways to a </span><span class="c4">legislative</span><span class="c0">&nbsp;client.</span></p><h3 class="c10" id="h.dgkksmp7wead"><span class="c2">European Variations of the OTA Paradigm</span></h3><p class="c3 c14"><span class="c0">OTA has always been the major example for establishing parliamentary technology assessment in Europe and elsewhere, but in practice none of the European organizations operates even remotely in the same way. Apart from the different political systems accounting for some of the deviations, there are also more mundane factors, not the least of which is--until recently--the existence of OTA itself and the large capacity of OTA in comparison to its European counterparts. OTA alone had an analytic capacity far outweighing the total capacity of European parliamentary technology assessment. Were a European organization to start up an OTA-like project, a substantive part of its capacity would be involved, and therefore only a very limited number of projects could be conducted in a certain </span><span class="c4">time frame</span><span class="c0">, thereby immediately limiting the scope of activities. Furthermore, with a limited capacity it is impossible to have in-house expertise in all the areas that a general technology assessment organization would have to cover. All European parliamentary technology assessment organizations finish a modest </span><span class="c4">number</span><span class="c0">&nbsp;of projects per annum relative to OTA. Some (e.g., the French and the German) address a small number of large issues at any time; whereas, others (e.g., the United</span><span class="c27">&nbsp;</span><span class="c0">Kingdom with its Briefing Notes and the Rathenau Institute with Messages to </span><span class="c4">Parliament</span><span class="c0">) combine smaller projects with larger ones.</span></p><p class="c3 c5"><span class="c0">But limiting the number of projects is not the only solution to solve the capacity problem. Technology assessment organizations have been borrowing parts of OTA assessments, adapting them to fit local needs, and thereby limiting the need for original analysis [20]. 7</span></p><p class="c3 c14"><span class="c0">As a consequence of their limited size, European technology assessment </span><span class="c4">organizations</span><span class="c0">&nbsp;have emphasized the &quot;timeliness&quot; of activities over the &quot;thoroughness&quot; of analysis. POST is a case in point, especially during the years in which it was still more limited in funds than it is now. During a number of years it operated mainly via its short Briefing Notes, providing Parliamentarians with factual information on actual developments. A close relationship to the scientific community in the United Kingdom enabled POST to generate the needed information quickly. The main characteristic of its way of operating is its supportiveness to Parliamentarians in their daily operations. It is difficult to gauge whether this type of information could have played a similar role in the United States, with its much more adversarial political culture and the much stricter separation of powers.</span></p><p class="c3 c5"><span class="c0">Especially in Denmark and the Netherlands, however, there are different and more fundamental reasons why the OTA paradigm did not become a standard, and certainly not the only standard. These reasons are related to the role that technology assessment is considered to have as a means for rendering decision making more democratic, and to the developing research area of technology studies as a way of thinking about technological development. Although these factors are most obvious in the Netherlands and Denmark, the underlying tendencies play a role in other countries, and therefore it is relevant to discuss these in some depth. I will do so by describing them as two more technology assessment paradigms.</span></p><h3 class="c10" id="h.b897bm5id409"><span class="c2">PUBLIC TECHNOLOGY ASSESSMENT</span></h3><p class="c3 c14"><span class="c0">In the development of parliamentary technology assessment, not only did the balancing of power between the executive and the legislature play a role, but wider thoughts of the democratic control of technology did as well. There is a widely held feeling that technology assessment is meant to empower democracy in influencing the directions taken in technological development. For example, Joe Coates made this connection when he discussed OTA at the opening session of the first European </span><span class="c4">technology</span><span class="c0">&nbsp;assessment conference in 1987 [21]:</span></p><p class="c3 c24"><span class="c12">In short, technology assessment is most appropriately, even necessarily, directed at public rather than private decision making, and still more appropriately at legislative rather than executive </span><span class="c27">decision making</span><span class="c12">. It is particularly in the legislative arena that all interests are represented, that all values can be defended and attacked, that conflicts can be negotiated, and balances struck .... The legislature is, in theory, a forum of representative laymen---certainly laymen in the context of science and technology .... An OTA is one bridge between experts and the public forum, the translator of technical information into public language for debate and decision.</span></p><p class="c3 c7"><span class="c0">One of the most important issues is the role of representation in the democratic process. In the above quote by Coates, Congress is viewed as the representative forum of the American people, and OTA has the role to provide this forum with information needed for fulfilling its task in the area of science and technology. In other countries, however, (and in other contexts in the United States, for that matter) much more</span><span class="c4">&nbsp;</span><span class="c0">emphasis is placed upon a lack of interaction among experts, representatives, and a lay public with respect to science and technology issues. Controversies over technologies are seen much less as a balancing act between the executive and the legislative, than as a problem between the government and the populace.</span></p><p class="c3 c6"><span class="c0">Some public technology assessment organizations are charged with bridging that gap, especially the Danish Technology Board and NOTA/the Rathenau Institute. In addition to conducting assessments of technological developments, the Danish Board of Technology also has the mission to foster public debates on the scope and implications of technology. NOTA was started in the context of the Dutch government&#39;s attempt to broaden the basis for decision making about technology. One of the tasks associated with this attempt was to &quot;open up a societal address-function,&quot; envisioned as a kind of mailbox where people could submit their worries about technological developments. The implicit argument was that society would not reap all the possible fruits of technological developments unless technology could be well-embedded in society. Although the </span><span class="c4">societal</span><span class="c0">&nbsp;address-function was only marginally realized, expanding the relationship between people and technology has remained an important motive for TA in the public realm, and it is part of the explanation of why fostering public debate became so central in </span><span class="c4">Rathenau</span><span class="c0">&nbsp;mission.</span></p><p class="c3 c6"><span class="c0">The Board of Technology in Denmark developed a standard procedure to foster debate on the scope and implications of technology in the form of consensus conferences. In these conferences, a panel of lay people is thoroughly educated about a certain technological development, such as the genetic modification of animals. After discussing the various viewpoints with experts invited by them and with a larger audience, they write down their conclusions in a consensus document, which then is delivered to parliament as a basis for further policy development.</span></p><p class="c3 c5"><span class="c0">In Denmark, the consensus document is indeed viewed as an important input for policy, and the consensus development process is seen as a way of involving the Danish people in the technology debate. It fits into a long tradition of consulting the population. In countries other than Denmark, however, the final document of a consensus conference does not hold the same importance: it is viewed as only one input in the decision process that should not circumvent the prerogatives of democratically chosen representatives. This is clearly visible in the debate on consensus conferences in the Netherlands, where the importance of the final document seems to be to get an overview of laypeople&#39;s views, rather than an actual consensus. For this reason consensus conferences have even be re-named &quot;public debates&quot; [22]. </span><span class="c4 c33">8</span></p><h3 class="c10" id="h.g6qsv3ixv35x"><span class="c2">CONSTRUCTIVE TECHNOLOGY ASSESSMENT</span></h3><p class="c3 c6"><span class="c0">Early in the history of technology assessment, it became clear that future effects of technology are not easy to forecast, let alone direct. Historical examples of weird predictions having been made (like </span><span class="c4">dust swirls</span><span class="c0">&nbsp;as a negative effect of automobile use) or of forecasts missed (like the non-predicted problems caused by the persistence of DDT) abound. The Collingridge dilemma [17] is often used to refer to the fact that</span><span class="c4">&nbsp;</span><span class="c0">forecasting unknown future effects of technology is difficult, whereas well-developed technology is difficult to direct, because it has become embedded in society (&quot;</span><span class="c4">entrenched</span><span class="c0">&quot; in the words of Collingridge). The Collingridge dilemma points to the fact that the early warning function of technology assessment has severe limitations, because either the knowledge or the power are missing to change the direction of technological development, leaving quick adaptations to new technology as the only way society can react. To a certain degree this viewpoint has been challenged by studies of the dynamics of technology development, pointing at the evolutionary development of technological trajectories. This in turn has led to the notion of influencing the development of </span><span class="c4">technology</span><span class="c0">&nbsp;by constructive technology assessment. Schot and Rip (this issue) describe </span><span class="c4">constructive</span><span class="c0">&nbsp;technology assessment as a new design practice--which include tools--in which impacts are anticipated, users and other impacted communities are involved from the start, and in an iterative way that contains an element of social learning.</span></p><p class="c3 c14"><span class="c0">The Dutch Science Dynamics program was the start of a development that </span><span class="c4">ultimately</span><span class="c0">&nbsp;led to constructive technology assessment. In the Netherlands, quite some effort was expended by the Minister of Science Policy 9 to initiate a research program directed at finding ways in which research can be oriented toward societal goals: the Science Dynamics program. This program became closely connected to international academic programs in the history and sociology of science and, after an initial phase, the people involved in the program started to delve into the dynamics of technological development. The students of technology dynamics sought ways to influence the direction of </span><span class="c4">technological</span><span class="c0">&nbsp;development. They share the view taken by neo-Schumpeterian economics that technological change is an endogenous process: technologies are conceived, developed and diffused by means of long and costly investments that are realized under economic and societal constraints [23]. Technology is not something &quot;out there,&quot; but evolves in close interaction with societal systems. Not only is the decision to develop technology through investing in research seen by modern economic theory--the so-called </span><span class="c4">endogenous</span><span class="c0">&nbsp;growth theory--as an economic choice like any other and therefore subject to &quot;normal&quot; economic considerations (a branch of analysis sometimes overlooked too much by students of technology dynamics); but it is also the interplay among many groups (within or between companies, or with regulatory agencies, for instance), </span><span class="c4">perceptions</span><span class="c0">&nbsp;(e.g., in R&amp;D and in markets) and institutions that result in the choice of a technological option and the dismissal of others. Of course, this process is not totally voluntaristic. The multiple competitive efforts of a great many global players results in a kind of &quot;invisible hand&quot; that informs and embeds particular product, process, or system developments. But it is clear that better knowledge of how the complex interplay of various actors molds specific technological developments, helps to influence </span><span class="c4">technological</span><span class="c0">&nbsp;development. Schot [24] has pointed at a number of ways in which this can be done.</span></p><p class="c3 c6"><span class="c0">This line of thinking led to the idea of constructive technology assessment, an active, positive form of shaping technological development in reaction to the original &quot;early warning&quot; approach [10, 25]. In the international debate on technology assessment, constructive technology assessment is sometimes viewed as the most important </span><span class="c4">contribution</span><span class="c0">&nbsp;from the Netherlands [10].</span></p><p class="c3 c6"><span class="c0">The body of literature about constructive technology assessment leads one to think how technological development can most effectively be influenced. It is not surprising that this should be done close to where technology development actually takes place,</span><span class="c4">&nbsp;</span><span class="c0">that is, primarily in industry. For this reason critics of constructive technology assessment sometimes view it as a form of enlightened management, of broadening the factors taken into account in the usual design processes in industry. Schot [26] emphasizes that such broadening as such is not a criterion for enhanced quality of technological development, but that it is only when anticipation, reflection, and learning take place that industrial development processes will result in better technology--technology with more positive and fewer negative effects. Proponents of constructive technology </span><span class="c4">assessment</span><span class="c0">&nbsp;therefore emphasize its effect on technological development because of the </span><span class="c4">possibility</span><span class="c0">&nbsp;that it changes the way in which the development process is shaped, taking into account considerations that otherwise would not have had enough leverage.</span></p><h3 class="c10" id="h.p0cfrukl9zdv"><span class="c2">PUBLIC AND CONSTRUCTIVE TECHNOLOGY ASSESSMENT COMPARED</span></h3><p class="c3 c5"><span class="c0">Consensus activities and constructive technology assessment both spring from the idea that the basis for decision making about technology should be broadened. Both also share the conviction that interaction among actors is important in conducting assessments. But, although closer links are often suggested, I do believe that these do not necessarily exist. Consensus conferences and the like emerge from a specific ideal of participatory democracy, and they can therefore be seen as a kind of public technology assessment; whereas, the main point of constructive technology assessment is influencing technological choice. These two directions may entail very different viewpoints and make use of very different bodies of knowledge.</span></p><p class="c3 c8"><span class="c0">In this section I sketched four paradigms of conducting technology assessments. The classical paradigm emphasized early warning and the neutral character of the information to be provided. In the United States this paradigm was replaced by the OTA paradigm, in which careful balancing of participation of Congress, stakeholders, and academia provided a mechanism leading to authoritative reports. In Europe, these reports and the underlying process are widely regarded as the optimal way of conducting technology assessment, but European organizations made adaptations to the OTA model. The reasons were partly practical--constraints of size and expertise--leading to more emphasis on timeliness than on thoroughness, and to a variety of ways to involve outside expertise. But part of the activities of some European technology assessment organizations are conducted according to different views of technology assessment, with a different emphasis. These I called the paradigms of public and constructive technology assessment, respectively. In these paradigms, the emphasis is much less on the production of authoritative reports than on social processes that may help shape technology in society: participation of a wider public (in public technology assessment) or influencing technological development by taking wider considerations into account (in constructive technology assessment).</span></p><p class="c3"><span class="c4">&nbsp; &nbsp;</span><span class="c0">All three modern views suggest that the original, purely analytic character of technology assessment has been replaced by a view that gives credit to the process of technology assessment as much as to the analytical product. As phrased by Smits and Leijten [1]: &quot;Technology assessment is a process consisting of analyses of technological development and its consequences and of debate in relationship to these consequences.&quot; </span></p><p class="c3"><span class="c4">&nbsp; &nbsp;</span><span class="c0">It is also clear, however, that a huge difference exists in the various ways in which the relationship between analysis and debate is viewed in these three paradigms: from interactions as a mechanism to enhance the quality of assessments, to participation as the primary objective of technology assessment activities.</span></p><p class="c3 c14"><span class="c0">Especially in Denmark and the Netherlands, consensus activities and constructive technology assessment have become part of the mainstream parliamentary technology</span><span class="c4">&nbsp;</span><span class="c0">assessment, which is also reflected in the missions of the national &quot;parliamentary&quot; organizations for technology assessment. This should, however, not be interpreted to mean that consensus activities and constructive technology assessment are the only examples of parliamentary technology assessment in those countries. Activities fitting in the other paradigm occur as well, just as consensus activities and constructive technology assessment exist elsewhere, but related to other institutions. Yet difference in visibility certainly has to do with the way these &quot;new&quot; technology assessment activities have been institutionalized in Denmark and the Netherlands.</span></p><h3 class="c10" id="h.87azk5hmo5xq"><span class="c2">Lessons for Technology Assessment</span></h3><p class="c3 c14"><span class="c0">Are there any lessons for technology assessment to be drawn from the histories sketched above, acknowledging that the widespread praise for the quality of OTA reports nonetheless did not prevent its demise? My view is that technology assessment faces a number of dilemmas related to the issues that became visible in the genesis of technology assessment organizations and in the different paradigms that prevail. They have to do with the distance of technology assessment from the political process, its relationship to academia, and the questions of which societal processes technology assessment supports.</span></p><h3 class="c10" id="h.mibcx8ukqwvr"><span class="c2">DILEMMAS</span></h3><p class="c3 c7"><span class="c0">One important dilemma for the future of technology assessment springs from the fact that there is a multitude of perspectives, with the only common denominator being that technology assessment should help remedy shortcomings in the relationship between technology and society. Technology assessment activities exist by the grace of the complexity and opacity of the technology-society interface. Technology assessments can help to illuminate parts of that relationship, and they can be effective as long as enough parties involved view the effects as positive, because technology assessment generates options that provide new opportunities in the technical or social sense (this may include options that are not new, but become more legitimate through the </span><span class="c4">assessment</span><span class="c0">&nbsp;activity). Technology assessment may shift the views of some decision makers (not only politicians, but also others, such as those in industry) for whom options are optimal. By doing so, it may have the effect of changing the options chosen, either directly, or in the long run by shifting perspectives. But it is only by making perspectives or options visible that it may influence these choices. It does not directly influence the power base behind the choices, and therefore actors may always conclude that they are not helped (or not helped effectively enough) by such additional information (as apparently the Republicans did in the United States).</span></p><p class="c3 c14"><span class="c0">Additionally there are different viewpoints in society about the most important shortcomings in the technology-society relationship, how such shortcomings interact, and--consequently--of the remedies to be invoked. The poor information base of elected representatives, too little public involvement, or the narrowness of criteria taken into account in technological development, may all be perceived as the main shortcoming. In Europe, those options led to variation in the way in which parliamentary technology assessment is conducted, with the adapted OTA paradigm, public technology assessment, and constructive technology assessment as the most visible solutions.</span></p><p class="c3 c6"><span class="c0">A second dilemma for technology assessment, and certainly for its parliamentary variety, is that the activity is essentially an interface between science and policy and that it thus faces all the problems of science as well as those of policy. The success of technology assessment depends critically on being viewed as a valuable contribution to</span><span class="c4">&nbsp;</span><span class="c0">policy; whereas, it must at the same time be viewed as generating results that are scientifically valid. The dilemmas posed here are visible in the institutionalization process of almost every parliamentary technology assessment organization: How to be relevant to policy, but not subservient to the political system? How to conform to scientific quality rules without at the same time becoming irrelevant to policy?</span></p><h3 class="c10" id="h.7qxyntkret7u"><span class="c2">FUTURE DIRECTIONS</span></h3><p class="c3 c34"><span class="c0">The sketch provided of the present state of technology assessment does not </span><span class="c4">immediately</span><span class="c0">&nbsp;lead to a picture of its future, but it does generate an agenda for consideration. It is important that technology assessment organizations face up to the dilemmas de- scribed. If the demise of OTA teaches one lesson, it is that the existence of a technology assessment organization cannot be taken for granted. Furthermore, the fact that OTA no longer exists means that its reports can no longer be taken as the starting point for other organizations.</span></p><p class="c3 c5"><span class="c0">More than ever, it is important for technology assessment organizations to ask themselves what makes them effective in reducing shortcomings in the technology- society interface. There does not seem to be a general answer to this question. Answers provided depend upon the country as well as on the actors who answer the question. It is clear that, in the final analysis, a technology assessment organization must be viewed by others as making a difference in the perspectives and options involved in technological choice. Whether the emphasis should be on contributing to the information base of representatives, on enhancing participation of either stakeholders or lay people, or on shifting the criteria taken into account in technological choice depends on the role attributed to the organization.</span></p><p class="c3 c5"><span class="c0">But there is a complication because the interrelationship between the information base of representatives, participation, and choices in technology is less than clear. Therefore, in many cases, it will be valuable to take an eclectic view and to choose from a variety of approaches, depending upon the most glaring inadequacy in the situation. Lack of legitimacy of a policy calls for a different approach than lack of available options.</span></p><p class="c3 c14"><span class="c0">For a parliamentary technology assessment organization, whatever other goals an assessment may have, it should also support the information base of the elected representatives. Therefore, consensus activities should not be considered an activity of parliamentary technology assessment organizations unless they duly report conclusions and possible consequences to the representatives. A parallel can be drawn for </span><span class="c4">constructive</span><span class="c0">&nbsp;technology assessment, However, given the unclear relationship between the ways options become part of the basis for choice at different levels in society, it is worthwhile to experiment with those approaches to enhance the societal learning processes about these choices.</span></p><p class="c3 c6"><span class="c0">Of immediate relevance is also enhancing the process of quality control. OTA had an extensive and visible quality control process; European parliamentary technology assessment organizations do have such processes, but they tend to be less well developed and certainly less visible. OTA&#39;s quality control procedures were important in gaining respectability for technology assessment. I think there is a need to develop and </span><span class="c4">systematize</span><span class="c0">&nbsp;a quality control system, one that does justice to the character of technology assessments between research and politics. After the demise of OTA, the development of a widely accepted way of appraising the quality of assessments becomes more </span><span class="c4">important</span><span class="c0">&nbsp;than ever, because the &quot;standard&quot; set by OTA no longer exists and one can no longer refer to it, even implicitly.</span></p><p class="c3 c5"><span class="c0">A quality control system for technology assessments could start from the review process or the criteria to assess the quality of final products. Both issues were addressed in the Westermeyer evaluation of STOA [3]. The Westermeyer report suggested the following quality assurance procedure for consideration in STOA:</span></p><ul class="c25 lst-kix_oz3l4i5d4rj3-0 start"><li class="c3 c17"><span class="c0">the establishment of external advisory panels for all major technology </span><span class="c4">assessment</span><span class="c0">&nbsp;projects;</span></li><li class="c3 c17"><span class="c0">the use of independent outside experts to review more complete drafts of report</span><span class="c4">s</span><span class="c0">;</span></li><li class="c3 c17"><span class="c0">implementation of a requirement that draft and final reports be reviewed by at least three people within the organization, and that specific comments be communicated to contractors for revisions; and</span></li><li class="c3 c17"><span class="c0">the hiring of a full-time editor to improve the readability and presentation of contractor and staff reports before release to Parliament.</span></li></ul><p class="c3 c14"><span class="c0">The adoption of such requirements is especially important in the small and floating environments of many technology assessment organizations. The suggestion to create such a process was based on the evaluation of STOA&#39;s reports. The Westermeyer report considered five criteria to be of primary importance in evaluating assessments. Objectivity was considered both in the context of the comprehensiveness and balance of reports and with respect to the accuracy of the information presented. The three other important criteria were the readability of the reports to the intended audience, the adequacy with which options were assessed and identified, and the relevance of reports to the work of the European Parliament.</span></p><p class="c3 c6"><span class="c0">These criteria and the process suggested to enhance the quality of the technology assessment output were developed as a practical approach to address the question of quality assurance. The criteria discussed can be used as a starting guideline. But the challenge for technology assessment practitioners is to elaborate such a quality assurance system in a more systematic way. One useful starting point for such a quality control system could be the approach taken by Clark and Majone [27] for the critical appraisal of inquiries with policy implications.</span></p><p class="c3 c5"><span class="c0">Clark and Majone tried to address systematically the questions raised about quality in science-for-policy. They begin by arguing that the appraisal of the input (data, methods, people engaged in the inquiry), the process (institutional structures and </span><span class="c4">procedures</span><span class="c0">, provisions for quality control, questions of standing that govern participation in and conduct of the inquiry), and the output (scientific facts, problem solutions, </span><span class="c4">conclusions</span><span class="c0">) of such inquiries is needed. They offer four criteria to apply in such appraisals: adequacy, value, effectiveness, and legitimacy. Adequacy may be practically defined as avoiding pitfalls in the use of data and methods, and in organizing the information provided in well-defined categories. Value is related to the relevance issue addressed above. The effectiveness criterion addresses the question of whether efforts undertaken to help resolve problems actually do so. Legitimacy in policy-related analysis has to be evaluated from the political perspective of politics as well as science. By focusing on such systematic attempts at appraisal, the future of technology assessment may find its way around these dilemmas.</span></p><h3 class="c10" id="h.2fumb5f40b1x"><span class="c2">References</span></h3><p class="c9"><span class="c0">1. Smits, R., and Leijten, J.: Technology Assessment, Waakhond of Speurhond?, Kerckebosch, Zeist, 1991,</span><span class="c4">&nbsp;</span><span class="c0">pp. 264, 340. </span></p><p class="c9"><span class="c0">2. Smits, R.: State of the Art of Technology Assessment in Europe, A Report to the 2rid European Congress</span></p><p class="c9"><span class="c0">on Technology Assessment, Mailand (14-16 November 1990).</span></p><p class="c9"><span class="c11 c4">3. Westermeyer, W.: An Evaluation of the Scientific and Technological Options Programme, A Report to the</span><span class="c4 c21">&nbsp;</span><span class="c0">European Parliament, PE 164.968, Luxembourg (October 1994), p. 1, 2. </span></p><p class="c9"><span class="c0">4. Vig, N. J.: Parliamentary Technology Assessment in Europe: Comparative Evolution, Impact Assessment</span><span class="c4">&nbsp;</span><span class="c4 c11">Bulletin 10(4), 3-24 (1992). </span></p><p class="c9"><span class="c11 c4">5. Tweede Kamer: </span><span class="c4 c21">integratie</span><span class="c11 c4">&nbsp;van Wetenschap en Technologie in de Samenleving (Integration of Science and </span><span class="c0">Technology in Society), white paper, parliamentary year 1983-1984, 18 421 hrs. 1-2.</span></p><p class="c9"><span class="c0">6. Commissie voor de Evaluatie van de Nederlandse Organisatie voor Technologisch Aspectenonderzoek: NOTA in discussie, Een evaluatie van de Nederlandse Organisatie voor Technologisch Aspectenonderzoek (Evaluation report of NOTA), Koninklijke Nederlandse Akademie van Wetenschappen, Amsterdam. December 1992. </span></p><p class="c9"><span class="c0">7. Staff: Denmark: The Technology Board Attached to the Danish Folketing, in European </span><span class="c4">Parliamentary</span><span class="c0">~ Technology&#39; Assessment--EPTA--Description of the Organizations Participating in the EPTA Coordination Network, 1990, An information Report to the 2nd European Congress of Technology Assessment, Mailand. 14-16 November 1990, p. 24-27. </span></p><p class="c9"><span class="c0">8. Hansen, L.: The Danish Board of Technology Established on a Permanent Basis, TA-Datenbank Nachrichten 4(4), 17-18 (1995). </span></p><p class="c9"><span class="c0">9. Staff: European Parliament, The STOA Programme, in European Parliamentary Technology Assessment--- EPTA--Description of the Organizations Participating in the EPTA Coordination Network, 1990, An information Report to the 2nd European Congress of Technology Assessment, Mailand, 14-16 November 1990, pp. 4-7. </span></p><p class="c9"><span class="c0">10. Vig, N. J.: Parliamentary Technology Assessment in Europe: A Comparative Perspective, in Science,</span><span class="c4">&nbsp;</span><span class="c0">Technology and Politics: Policy Analysis in Congress. G. C. Bryner, ed., Westview, Boulder, 1992. </span></p><p class="c9"><span class="c0">11. Jaeger, D.: Germany: The Bundestag Committee of Inquiry on Technology Assessment and Evaluation, in European Parliamentary Technology Assessment--EPTA--Description of the Organizations Participating in the EPTA Coordination Network, 1990, An information Report to the 2nd European Congress of Technology Assessment, Mailand, 14-16 November 1990, pp. 36~2. 12. Petermann, T.: </span><span class="c4">Technikfolgenabsch&auml;tzung</span><span class="c0">&nbsp;beim Deutschen Bundestag wird zu einer </span><span class="c4">st&auml;ndigen</span><span class="c0">&nbsp;Aufgabe- </span><span class="c4">Verl&auml;ngerung</span><span class="c0">&nbsp;des TAB im </span><span class="c4">Bundestag</span><span class="c0">&nbsp;einstimmig beschlossen. TA-Datenbank-Nachrichten 2(2), 16-17 (1993). </span></p><p class="c9"><span class="c0">13. Coenen, R., Von Berg, I., and Schevitz, J.: TA-Monitoring </span><span class="c4">Bericht</span><span class="c0">&nbsp;L </span><span class="c4">Parlamentarische</span><span class="c0">&nbsp;TA-</span><span class="c4">Einrichtungen Und</span><span class="c0">&nbsp;ihre </span><span class="c4">Gegenw&auml;rtigen</span><span class="c0">&nbsp;Themen, TAB- Arbeitsbericht 5/91, September 1991, Karlsruhe. </span></p><p class="c9"><span class="c0">14. </span><span class="c4">Beckman</span><span class="c0">, G.: Technology Assessment: Democratic Function of Technology Assessment in Technology</span><span class="c4">&nbsp;</span><span class="c11 c4">Policy Making, Science and Public Policy 20(1), 11-16 (1993).</span></p><p class="c9"><span class="c11 c4">15. Schwartz, M., and Thompson, M.: Divided We Stand: Redefining Politics. Technology and Social Choice,</span><span class="c0">Harvester Wheatsheaf, New York, 1990, </span></p><p class="c9"><span class="c0">16. Coates, V. T.: Readings in TA, George Washington University, Washington, DC, 1975. </span></p><p class="c9"><span class="c0">17. Collingridge, D.: The Social Control of Technology, Frances Pinter, London, 1980. </span></p><p class="c9"><span class="c0">18. Procter, M.: Probing the Core of Controversy: Issue Management in the Office of Technology Assessment, in Technology Assessment, An Opportunity for Europe. S. C. De Hoo. R. E. H. M. Smits, and R. Petrella, eds., The Hague, 1987, pp. 111-120. </span></p><p class="c9"><span class="c0">19. Smits, R. E. H. M.: Technology Assessment: An Opportunity for Europe; Aspects of the Integration of Science and Technology in American Society, Publication of the Dutch Ministry of Education and Science in cooperation with the Commission of the European Communities (DG XII)/FAST,</span><span class="c4">&nbsp;</span><span class="c0">Government Printing Office, The Hague, January 1987, p. 30. </span></p><p class="c9"><span class="c0">20, U.S. Congress, Office of Technology Assessment: Critical Connections: Communications .for the Future,</span><span class="c4">&nbsp;</span><span class="c0">OTA-C1T-407, U.S. Government Printing Office, Washington, DC, 1990. </span></p><p class="c9"><span class="c0">21. Coates, J.: Technology Assessment in the United States Congress, in Technology Assessment, An </span><span class="c4">Opportunity</span><span class="c0">&nbsp;for Europe. S. C. De Hoo, R. E. H. M. Smits, and R. Petrella, eds., The Hague, 1987, pp. 31-38. </span></p><p class="c9"><span class="c0">22. Hennen, L.: Konsens ueber Konsensus Konferenzen? TA-Datenbank Nachrichten 3(4), 75-76 (1995). </span></p><p class="c9"><span class="c0">23. Rip, A., Misa, T. J., and Schot, J.: Managing Technology in Society: The Approach of Constructive Techmd.</span><span class="c4">&nbsp;</span><span class="c11 c4">ogy Assessment, Pinter, London, 1995, p. 16. 24. Schot, J.: Technology Dynamics: An Inventory of Policy Implications for Constructive Technology Assess</span><span class="c0">ment, NOTA Working Document 45, The Hague, 1991. </span></p><p class="c9"><span class="c0">25. Schot, J.: Constructive Technology Assessment and Technology Dynamics: The Case of Technology Dynam</span><span class="c11 c4">ics, Science,</span><span class="c4 c21">&nbsp;</span><span class="c11 c4">Technology and Human Values 17(1), 36-56 (1991).</span></p><hr style="page-break-before:always;display:none;"><p class="c9"><span class="c0">286 J.C.M. VAN EIJNDHOVEN</span></p><p class="c9"><span class="c0">26. Schot, J.: De inzet van Constructief Technology Assessment, Kennis en Methode 20(3), 265-293 (1996). 27. Clark, W. C., and Majone, G.: The Critical Appraisal of Scientific Inquiries with Policy Implications,</span></p><p class="c9"><span class="c11 c4">Science, Technology and Human Values 10(3), 6-19, (1985).</span></p><p class="c9"><span class="c11 c4">Received 18 August 1996; accepted 12 November 1996</span></p><p class="c9 c29"><span class="c11 c4"></span></p><p class="c3"><span class="c0">JOSI~E VAN EIJNDHOVEN is professor of technology assessment at the Department of Science, Technology, and Society of Utrecht University (Netherlands) and director of the Rathenau Institute for technology assessment at The Hague.</span></p><p class="c9"><span class="c0">Address correspondence to Jos6e C. M. van Eijndhoven, Rathenau Institute, P.O. Box 85525, 2508 CE, The Hague, The Netherlands.</span></p><p class="c9"><span class="c0">Technological Forecasting and Social Change 54, 269-286 (1997) &copy; 1997 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010</span></p><p class="c9"><span class="c0">0040-1625/97/$17.00 PII S0040-1625(96)00210-7</span></p><p class="c3"><span class="c1">1</span><span class="c27">&nbsp;A comparative analysis of the institutionalization and the methodologies of each of these organizations is the subject of a research project coordinated by Norman Vig, Carleton College, Minnesota. I restrict myself to a small number of aspects.</span></p><p class="c9"><span class="c1">2</span><span class="c12">&nbsp;Presentation on OPECST in the frame of the Vig project (see Footnote 1), May 31-June 1, 1996. </span></p><p class="c9"><span class="c1">3</span><span class="c12">&nbsp;In Dutch, technology assessment is translated as &quot;Technologisch Aspectenonderzoek.&quot; The word &quot;onderzoek&quot; is used for &quot;research&quot; as well as &quot;analysis,&quot; and thus its connotation is very much one of an academic activity.</span></p><p class="c9"><span class="c1">4</span><span class="c12">&nbsp;See the article by Norton in this issue for further discussion of the UK Parliamentary Office of Science and Technology.</span></p><p class="c9"><span class="c1">5</span><span class="c12">&nbsp;These Parliamentarians included Margaret Thatcher, who opposed a role for POST as a formal body of Parliament.</span></p><p class="c3"><span class="c1">6</span><span class="c12">&nbsp;The intimate relationship of OTA to Congress and of the formal procedures to begin technology assessments rendered it impossible for OTA to start inquiries before members of Congress realized the need for additional analysis in certain areas. Such strong ties to elected representatives with a relatively short time horizon is a severe limitation for an institute that is supposed to analyze future developments.</span></p><p class="c9"><span class="c1">7</span><span class="c12">&nbsp;An example is the use of the OTA report, Critical Connections [20], which was extensively used in the Netherlands in its debates over telecommunications policy.</span></p><p class="c3"><span class="c1">8</span><span class="c12">&nbsp;The same reflections on the relationship of consensus activities to chosen representatives are made in other European countries [22]. Another contrast between the role of consensus conferences in Denmark and the Netherlands is the way the media view them. In Denmark, consensus conferences seem to get much more media attention. In the Netherlands, however, these activities do lead to an upsurge of attention to the technological development being discussed, but only very partial attention to the debate itself. Science journalists hold that &quot;no new information is likely to spring from these activities.&quot; The media see it as their prerogative to support public debate.</span></p><p class="c9"><span class="c1">9</span><span class="c12">&nbsp;There was a Minister of Science Policy from 1973 to 1981.</span></p><div><p class="c16"><span class="c22 c35"></span></p></div></body></html>